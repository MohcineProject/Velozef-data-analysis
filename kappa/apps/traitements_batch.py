import os
from pyspark.sql import SparkSession
from pyspark.sql.functions import lit
from datetime import datetime, timedelta
from functools import reduce

#### PARAMS ####
PARQUET_PATH = "parquet/station_status"
CURRENT_TIME = datetime.now()
HOURS_TO_LOAD = 24  # Nombre d'heures √† charger

# Initialisation de Spark
spark = SparkSession.builder \
        .appName("ReadLast24Hours") \
        .getOrCreate()

def get_existing_partitions(base_path, hours_to_load):
    """Retourne uniquement les partitions qui existent r√©ellement"""
    existing_paths = []
    for i in range(1, hours_to_load + 1):
        time = CURRENT_TIME - timedelta(hours=i)
        partition_path = f"day={time.strftime('%d')}/hour={time.strftime('%H')}"
        
        # V√©rifie si le chemin de partition existe
        full_path = os.path.join(base_path, partition_path)
        try:
            # Liste les sous-partitions existantes
            minute_partitions = [f.path for f in os.scandir(full_path) if f.is_dir()]
            if minute_partitions:
                existing_paths.extend(minute_partitions)
            else:
                print(f"‚ö†Ô∏è Aucune donn√©e pour {partition_path}")
        except FileNotFoundError:
            print(f"‚è≠Ô∏è Partition non trouv√©e: {partition_path}")
            continue
    
    return existing_paths

# 1. R√©cup√©ration des partitions existantes
existing_partitions = get_existing_partitions(PARQUET_PATH, HOURS_TO_LOAD)

if not existing_partitions:
    print("‚ùå Aucune partition trouv√©e pour les 24 derni√®res heures")
    spark.stop()
    exit()

# 2. Lecture des partitions existantes
try:
    dfs = []
    for path in existing_partitions:
        try:
            df_part = spark.read.parquet(path)
            # df_part = df_part.withColumn("data_source", lit(path))  # Ajoute le chemin source
            dfs.append(df_part)
        except Exception as e:
            print(f"‚ö†Ô∏è Erreur lecture {path}: {str(e)}")
            continue
    
    if not dfs:
        print("‚ùå Aucune donn√©e valide √† charger")
        spark.stop()
        exit()
    
    # Union de tous les DataFrames
    df = reduce(lambda a, b: a.union(b), dfs)
    
    # Statistiques
    print("\n‚úÖ Chargement r√©ussi")
    print(f"üìÇPartitions charg√©es: {len(dfs)}")
    print(f"Nombre total de lignes: {df.count()}")
    print("\n Sch√©ma:")
    df.printSchema()
    print("\n Aper√ßu des donn√©es:")
    df.show(5, truncate=False)
    
except Exception as e:
    print(f"‚ùå Erreur majeure: {str(e)}")
finally:
    spark.stop()